{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a82b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint..\n",
      "Initializing samples..\n"
     ]
    }
   ],
   "source": [
    "from trainer import *\n",
    "\n",
    "with open('configs/mac_test.yaml') as file:\n",
    "    config = yaml.full_load(file)\n",
    " \n",
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf752af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainer.dataset\n",
    "batch   = next(iter(trainer.train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f891d9",
   "metadata": {},
   "source": [
    "# Create a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb614ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_img  = batch['tgt']         # [b, 3, H, W]\n",
    "ref_imgs = batch['ref_imgs']    # [[b, 3, H, W], [b, 3, H, W]]\n",
    "K        = batch['intrinsics']  # [b, 3, 4]\n",
    "depth    = batch['groundtruth'] # [b, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bbae457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85195df",
   "metadata": {},
   "source": [
    "# The Inverse Warp\n",
    "\n",
    "The goal is to create a target image by transforming source images. But to get around splatting, we have to:\n",
    "\n",
    "1. Transform target to source\n",
    "2. As transformed pixels are continuous, use bilinear sampling (differentiable) to sample from source images.\n",
    "3. As we know which pixel in target corresponds to which \"transformed-and-sampled\" source image we can create a target image.\n",
    "\n",
    "### Resources\n",
    "- [X] https://www.youtube.com/watch?v=lNYhWBPEeaY&list=PLyqSpQzTE6M-T5ZrthkU763MHKIKCa0sX&index=7\n",
    "- [X] https://github.com/nianticlabs/monodepth2/issues/87\n",
    "- [ ] https://www.cse.huji.ac.il/course/2006/impr/lectures2006/Tirgul8_LK.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d52958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid(B, H, W, dtype, device, normalized=False):\n",
    "    \"\"\"\n",
    "    Create meshgrid with a specific resolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : int\n",
    "        Batch size\n",
    "    H : int\n",
    "        Height size\n",
    "    W : int\n",
    "        Width size\n",
    "    dtype : torch.dtype\n",
    "        Meshgrid type\n",
    "    device : torch.device\n",
    "        Meshgrid device\n",
    "    normalized : bool\n",
    "        True if grid is normalized between -1 and 1\n",
    "    Returns\n",
    "    -------\n",
    "    xs : torch.Tensor [B,1,W]\n",
    "        Meshgrid in dimension x\n",
    "    ys : torch.Tensor [B,H,1]\n",
    "        Meshgrid in dimension y\n",
    "    \"\"\"\n",
    "    if normalized:\n",
    "        xs = torch.linspace(-1, 1, W, device=device, dtype=dtype)\n",
    "        ys = torch.linspace(-1, 1, H, device=device, dtype=dtype)\n",
    "    else:\n",
    "        xs = torch.linspace(0, W-1, W, device=device, dtype=dtype)\n",
    "        ys = torch.linspace(0, H-1, H, device=device, dtype=dtype)\n",
    "    ys, xs = torch.meshgrid([ys, xs])\n",
    "    return xs.repeat([B, 1, 1]), ys.repeat([B, 1, 1])\n",
    "\n",
    "def image_grid(B, H, W, dtype, device, normalized=False):\n",
    "    \"\"\"\n",
    "    Create an image grid with a specific resolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : int\n",
    "        Batch size\n",
    "    H : int\n",
    "        Height size\n",
    "    W : int\n",
    "        Width size\n",
    "    dtype : torch.dtype\n",
    "        Meshgrid type\n",
    "    device : torch.device\n",
    "        Meshgrid device\n",
    "    normalized : bool\n",
    "        True if grid is normalized between -1 and 1\n",
    "    Returns\n",
    "    -------\n",
    "    grid : torch.Tensor [B,3,H,W]\n",
    "        Image grid containing a meshgrid in x, y and 1\n",
    "    \"\"\"\n",
    "    xs, ys = meshgrid(B, H, W, dtype, device, normalized=normalized)\n",
    "    ones = torch.ones_like(xs)\n",
    "    grid = torch.stack([xs, ys, ones], dim=1)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0904775",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_grid = image_grid(4, depth[0].shape[0], depth[0].shape[1], depth.dtype, depth.device, normalized=False).view(4, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4aaa2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kinv():\n",
    "    \"\"\"Inverse intrinsics (for lifting)\"\"\"\n",
    "    Kinv = K.clone()\n",
    "    Kinv[:, 0, 0] = 1. / K[:, 0, 0]\n",
    "    Kinv[:, 1, 1] = 1. / K[:, 1, 1]\n",
    "    Kinv[:, 0, 2] = -1. * K[:, 0, 2]/ K[:, 0, 0]\n",
    "    Kinv[:, 1, 2] = -1. * K[:, 1, 2]/ K[:, 1, 1]\n",
    "    return Kinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe11930b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3859e-03,  0.0000e+00, -8.4481e-01,  4.4857e+01],\n",
       "         [ 0.0000e+00,  1.3859e-03, -2.3956e-01,  2.1638e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  2.7459e-03]],\n",
       "\n",
       "        [[ 1.3859e-03,  0.0000e+00, -8.4481e-01,  4.4857e+01],\n",
       "         [ 0.0000e+00,  1.3859e-03, -2.3956e-01,  2.1638e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  2.7459e-03]],\n",
       "\n",
       "        [[ 1.3859e-03,  0.0000e+00, -8.4481e-01,  4.4857e+01],\n",
       "         [ 0.0000e+00,  1.3859e-03, -2.3956e-01,  2.1638e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  2.7459e-03]],\n",
       "\n",
       "        [[ 1.3859e-03,  0.0000e+00, -8.4481e-01,  4.4857e+01],\n",
       "         [ 0.0000e+00,  1.3859e-03, -2.3956e-01,  2.1638e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  2.7459e-03]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kinv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a382e8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0014,  0.0000, -0.8448],\n",
       "         [ 0.0000,  0.0014, -0.2396],\n",
       "         [ 0.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.0014,  0.0000, -0.8448],\n",
       "         [ 0.0000,  0.0014, -0.2396],\n",
       "         [ 0.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.0014,  0.0000, -0.8448],\n",
       "         [ 0.0000,  0.0014, -0.2396],\n",
       "         [ 0.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.0014,  0.0000, -0.8448],\n",
       "         [ 0.0000,  0.0014, -0.2396],\n",
       "         [ 0.0000,  0.0000,  1.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K[..., :3, :3].inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ecfc894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.2154e+02, 0.0000e+00, 6.0956e+02, 4.4857e+01],\n",
       "         [0.0000e+00, 7.2154e+02, 1.7285e+02, 2.1638e-01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 2.7459e-03]],\n",
       "\n",
       "        [[7.2154e+02, 0.0000e+00, 6.0956e+02, 4.4857e+01],\n",
       "         [0.0000e+00, 7.2154e+02, 1.7285e+02, 2.1638e-01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 2.7459e-03]],\n",
       "\n",
       "        [[7.2154e+02, 0.0000e+00, 6.0956e+02, 4.4857e+01],\n",
       "         [0.0000e+00, 7.2154e+02, 1.7285e+02, 2.1638e-01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 2.7459e-03]],\n",
       "\n",
       "        [[7.2154e+02, 0.0000e+00, 6.0956e+02, 4.4857e+01],\n",
       "         [0.0000e+00, 7.2154e+02, 1.7285e+02, 2.1638e-01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 2.7459e-03]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5619990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the outward rays in the camera frame\n",
    "xnorm = (Kinv.bmm(flat_grid)).view(B, 3, H, W)\n",
    "# Scale rays to metric depth\n",
    "Xc = xnorm * depth\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
